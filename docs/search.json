[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Meta-Analysis of Machine Learning in Remote Sensing",
    "section": "",
    "text": "Abstract\nIntroThe United Nations SDG report (2023) emphasises embracing new data sources and innovative methodologies, including remote sensing. To leverage these data sources and complement traditional survey data, machine learning can be applied. This growing field of research has an extensive body of literature, so selecting a suitable machine learning method poses a significant challenge. The following is a research proposal for a systematic review and meta-analysis to assess the variability of quality metrics and identify study features that may impact the performance of machine learning methods in remote sensing applications.\nMethods\nResults\nDiscustion",
    "crumbs": [
      "Abstract"
    ]
  },
  {
    "objectID": "writing/intro.html",
    "href": "writing/intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Previous research\nTable 1. List and a brief summary of previous reviews related to the topic.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "writing/intro.html#previous-research",
    "href": "writing/intro.html#previous-research",
    "title": "1  Introduction",
    "section": "",
    "text": "Studies have previously examined the application of remote sensing for SDG monitoring (see Yin et al. (2023); Holloway and Mengersen (2018);Burke et al. (2021) ). These reviews typically focus on one context (e.g. satellite imagery for poverty prediction (Hall et al. 2023) or focus on descriptive results (e.g. (Yin et al. 2023)). Some meta-analysis make comparisons to attribute accuracy improvements to individual features (see: Khatami, Mountrakis, and Stehman (2016a) ).\nthere is a significant gap in the literature…..",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "writing/intro.html#the-present-study",
    "href": "writing/intro.html#the-present-study",
    "title": "1  Introduction",
    "section": "The present study",
    "text": "The present study\n\nThe present work is a review systematically assessing the variability of machine learning methods in SDGs monitoring applications, with a specific focus on identifying study features that may impact their performance. Review studies are important for creating a comprehensive overview, assessing the accuracy of different methods, as many factors or study features can explain heterogeneity of quality metrics among individual studies. For example, the data collecting device (e.g. satellite), image type, algorithm applied, input features, pre- and post-processing techniques, the use of ancillary data.\nWith is in mind the aim of this review is to recreate a guideline with universal applicability to improve model accuracy though the election of appropriate algorithms\n\nsystematic review application of machine learning methods in remote sensing, particularly focusing on research related to the SDGs\nExplaining variability of observed accuracy (quality metrics) across different remote sensing applications. with a meta-analysis\ndevelop and apply a meta-regression model to identify study features (meta-features) that explain the variation in classification quality.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "writing/intro.html#section-1",
    "href": "writing/intro.html#section-1",
    "title": "1  Introduction",
    "section": "",
    "text": "Burke, Marshall, Anne Driscoll, David B. Lobell, and Stefano Ermon. 2021. “Using Satellite Imagery to Understand and Promote Sustainable Development.” Science 371 (6535): eabe8628. https://doi.org/10.1126/science.abe8628.\n\n\nChen, Guolong, Ruixia Yang, Xiangli Zhao, Lanyi Li, Lei Luo, and Honghao Liu. 2023. “Bibliometric Analysis of Spatial Technology for World Heritage: Application, Trend and Potential Paths.” Remote Sensing 15 (19): 4695. https://doi.org/10.3390/rs15194695.\n\n\nHall, Ola, Francis Dompae, Ibrahim Wahab, and Fred Mawunyo Dzanku. 2023. “A Review of Machine Learning and Satellite Imagery for Poverty Prediction: Implications for Development Research and Applications.” Journal of International Development 35 (7): 1753–68. https://doi.org/10.1002/jid.3751.\n\n\nHolloway, Jacinta, and Kerrie Mengersen. 2018. “Statistical Machine Learning Methods and Remote Sensing for Sustainable Development Goals: A Review.” Remote Sensing 10 (9): 1365. https://doi.org/10.3390/rs10091365.\n\n\nKhatami, Reza, Giorgos Mountrakis, and Stephen V. Stehman. 2016a. “A Meta-Analysis of Remote Sensing Research on Supervised Pixel-Based Land-Cover Image Classification Processes: General Guidelines for Practitioners and Future Research.” Remote Sensing of Environment 177 (May): 89–100. https://doi.org/10.1016/j.rse.2016.02.028.\n\n\n———. 2016b. “A Meta-Analysis of Remote Sensing Research on Supervised Pixel-Based Land-Cover Image Classification Processes: General Guidelines for Practitioners and Future Research.” Remote Sensing of Environment 177 (May): 89–100. https://doi.org/10.1016/j.rse.2016.02.028.\n\n\nMachicao, J., A. Ben Abbes, L. Meneguzzi, P. L. P. Corrêa, A. Specht, R. David, G. Subsol, et al. 2022. “Mitigation Strategies to Improve Reproducibility of Poverty Estimations From Remote Sensing Images Using Deep Learning.” Earth and Space Science 9 (8): e2022EA002379. https://doi.org/10.1029/2022EA002379.\n\n\nShi, Haiyang, Geping Luo, Olaf Hellwich, Mingjuan Xie, Chen Zhang, Yu Zhang, Yuangang Wang, et al. 2022. “Variability and Uncertainty in Flux-Site-Scale Net Ecosystem Exchange Simulations Based on Machine Learning and Remote Sensing: A Systematic Evaluation.” Biogeosciences 19 (16): 3739–56. https://doi.org/10.5194/bg-19-3739-2022.\n\n\nThapa, Aakash, Teerayut Horanont, Bipul Neupane, and Jagannath Aryal. 2023. “Deep Learning for Remote Sensing Image Scene Classification: A Review and Meta-Analysis.” Remote Sensing 15 (19): 4804. https://doi.org/10.3390/rs15194804.\n\n\nUN DESA. 2023. The Sustainable Development Goals Report 2023: Special Edition. The Sustainable Development Goals Report. United Nations. https://doi.org/10.18356/9789210024914.\n\n\nYin, Chun, Ningyezi Peng, Yuchen Li, Yuanyuan Shi, Shujuan Yang, and Peng Jia. 2023. “A Review on Street View Observations in Support of the Sustainable Development Goals.” International Journal of Applied Earth Observation and Geoinformation 117 (March): 103205. https://doi.org/10.1016/j.jag.2023.103205.\n\n\nZhang, Yuzhen, Jingjing Liu, and Wenjuan Shen. 2022. “A Review of Ensemble Learning Algorithms Used in Remote Sensing Applications.” Applied Sciences 12 (17): 8654. https://doi.org/10.3390/app12178654.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "methods.html",
    "href": "methods.html",
    "title": "2  Methods",
    "section": "",
    "text": "Data collection process & Data items",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "methods.html#meta-analysis",
    "href": "methods.html#meta-analysis",
    "title": "2  Methods",
    "section": "Meta-analysis",
    "text": "Meta-analysis\nThe aims of systematic review and meta-analysis is to combine, summarize, analyse and interpret available evidence regarding a defined field or research question. Traditionally, they aim to summarizes quantitative outcomes :\n\nie a single outcome variable has been measured in the same way across all study subjects.\nfor example the effect of a medication on…\nthis is refered to as an effect size\nthe selected effect size must be:\n\nComparable. the same meaning across all studies. Let us take math skills as an example again. It makes no sense to pool differences between experimental and control groups in the number of points achieved on a math test when studies used different tests. Tests may, for example, vary in their level of difficulty, or in the maximum number of points that can be achieved.\nComputable. We can only use an effect size metric for our meta-analysis if it is possible to derive its numerical value from the primary study. It must be possible to calculate the effect size for all of the included studies based on their data.\nReliable. Even if it is possible to calculate an effect size for all included studies, we must also be able to pool them statistically. To use some metric in meta-analyses, it must be at least possible to calculate the standard error (see next chapter). It is also important that the format of the effect size is suited for the meta-analytic technique we want to apply, and does not lead to errors or biases in our estimate.\nInterpretable. The type of effect size we choose should be appropriate to answer our research question. For example, if we are interested in the strength of an association between two continuous variables, it is conventional to use correlations to express the size of the effect. It is relatively straightforward to interpret the magnitude of a correlation, and many researchers can understand them. In the following chapters, we will learn that it is sometimes not possible to use outcome measures which are both easy to interpret and ideal for our statistical computations. In such cases, it is necessary to transform effect sizes to a format with better mathematical properties before we pool them.\n\n\n.In this case…. the studies address different subject matter…. this is referred to as the “applies and oranges” problem. Robert Rosenthal, a pioneer in meta-analysis, was questioned about the validity of conducting a meta-analysis when the studies involved have significant differences. He responded that it is sensible to combine apples and oranges if your goal is to create a fruit salad Borenstein (2013). Here studies with variable research aims are being collected however the aim is to make general comments about machine learning applications for remote sensed, SDG problems: fruit salad with universal applicability.\na more thorougher discussion of the limitations of these are is in ?sec-writing/discussion\n\nhttps://pubmed-ncbi-nlm-nih-gov.ezproxy.leidenuniv.nl/27620683/\nQuestionable Meta-research Practices\n\n\n“Apples and Oranges” Problem\n\n\nStudy risk of bias assessment\n\n\n\npapers that do not include confusion matrix, or have test set results, or….\nTO DO: look at: “utilize methodological quality and bias assessment tool (CLEAR-NPT, Coleman, Modified Coleman, CONSORT, Pedro, Cochrane, Delphi, Detsky, Downs and Black, Jadad, Level of evidence, MINORS, Newcastle-Ottawa, QUADAS, Quality Appraisal Tool, STARD, Strobe, AMSTAR, R-AMSTAR, etc.)”?\nhttps://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0234722#pone.0234722.s004\nhttps://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-023-01849-0\n\n\n\n\n\nEffect measures & Synthesis Methods\n\nOverall accuracy (OA) is defined as the probability of a correct classification (Magnussen 2021)\n\n\\[\n\\text{Overall Accuracy (OA)} =  \\frac{\\text{True Positive + True Negative}}{N}\n\\]\nproportion there for the standard error is\nstandard error: \\[\nSE_p =  \\sqrt \\frac{p(1-p)}{N} \\\\\n\\text{Where p is probability of a correct classification (OA)}\n\\]\n\n\nMeta analysis\nRQ: Is there heterogeneity between studies:\n“It is highly advised to specify the type of model you used in the methods section of your meta-analysis report.”\n\nexpect considerable between study heterogeneity\n\nuse random-effects model to pool the effect size\nTO DO: choose between methods to estimate variance \\(\\tau^2\\)\nusing {meta} package: metaprop for proportions:\n\nlogit-transform proportions before the meta-analysis is performed\napply a (generalized) logistic mixed-effect model to estimate the pooled effect\n\n\nmeasures of heterogeneity, forest plot\n…..\n\n\n\nMeta-Regression model\nRQ: What factors affect the probability of correct classification (Overall accuracy) of machine learning algorithms?\nMixed effects model:\n\\[\n\\hat{\\theta}_k = \\theta +\\beta_1 x_{1k} ... + \\beta_n x_{nk} + \\epsilon_k + \\zeta_k    \n\\]\nWhere \\(\\epsilon_k\\) sampling error deviation from the true effect size by the study \\(\\zeta_k\\) random effect… \\(x_i\\) are the explanatory variables (or features):….\nTO DO:\n\nrewrite as a probit model\nwrite about model fit assessments, choosing features (step-wise, or other) etc\nAnd which assumptions to check\n\n\n\n\n\nReporting bias assessment\n\n\nCertainty assessment\n\n\n\n\nBorenstein, Michael, ed. 2013. Introduction to Meta-Analysis. Nachdr. Chichester: Wiley.\n\n\nLajeunesse, Marc J. 2016. “Facilitating Systematic Reviews, Data Extraction, and Meta-Analysis with the Metagear Package for r” 7: 323–30.\n\n\nMagnussen, Steen. 2021. “Calibration of a Confidence Interval for a Classification Accuracy.” Open Journal of Forestry 11 (1): 14–36. https://doi.org/10.4236/ojf.2021.111002.\n\n\nPage, Matthew J, Joanne E McKenzie, Patrick M Bossuyt, Isabelle Boutron, Tammy C Hoffmann, Cynthia D Mulrow, Larissa Shamseer, et al. 2021. “The PRISMA 2020 Statement: An Updated Guideline for Reporting Systematic Reviews.” BMJ, March, n71. https://doi.org/10.1136/bmj.n71.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Methods</span>"
    ]
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "3  Results",
    "section": "",
    "text": "Overal dataset Distribution of papers over:\n\ntime\ncountry\n\n\n# read final dataset here \n\nX articles were included with Y model records for meta-analysis extracted. Most studies were from countries\n\n\n\nTable 3.1: A listing of extracted features considered for the content analysis of included articles\n\n\n\n\n\n\n\n\n\n\nFeature\nDefinition\nvalue/categories\n\n\n\n\nPaper ID\nDOI used as identification number of the article\n\n\n\nAuthor(s), year, DOI\nName(s) of authers\n\n\n\nTitle\nTitle of the article\n\n\n\nYear\nPublication Year\n\n\n\nPublication name\nName of journal that published paper\n\n\n\nLocation\nLocation of the data used (country level)\n\n\n\nResearch Theme\nArea of research\nTO DO: Add categories\n\n\nRS device\nType of RS device\nSatellite , Drone, Plane, Unmanned Aerial Vehicles (UAVs)\n\n\nRS device specifics\nSpecific device name\nSentinel-2, Landsat-8\n\n\nRS device\nGrouping?\n\n\n\nAncillary data\nUse of non-RS data in the model\n{0 = only remote sensed data, 1= additional data used} Additional (non- remote sensed data used)\n\n\nTraining/Test set\nSome confusion here if the test set or training set used, maybe I will add this as a feature!\ntest, train, not specified, ….\n\n\nRS bands\nlist of the bands used\n\n\n\nRS_spectral_bands_no\nnumber of bands used\n\n\n\nRS_spatital_resolution\nin meters\n\n\n\nmodel_group\nAlgorithm group used\nRandom forest (RF), artificial neural network (ANN), support vector machine (SVM),\n\n\nnumber_classes\nThe number of classes to predict\n\n\n\nfraction_majority_class\nThe fraction of the largest class\n\n\n\nTotal\nSample size\n\n\n\nAccuracy metric\nMeasure used to assess the predictive performance of the ML method applied\nOverall Accuracy (OA)\n\n\n\n\n\n\n\nDiscriptives stats\n\nStudy selection\n\nwhich algorithms performed better (look if pairwise comparison is possible\ncountries\njournals\ndoes OA increase with publication date\n\n\n\nStudy characteristics\n\ntable of article included (I think this will go in the appendix)\ntable: description of information extracted from the included papers\ntotal number papers\ndistributions of features\nmaybe pair plot to show associations between features\n\n\n\nMeta-Analysis Results:\n\npooled effect sizes and confidence intervals.\nmaybe with forest plots to visualize the individual study estimates (I have multiple results from one paper so maybe not the best method)\noverall effect size\ndegree of heterogeneity\n\n\n\nMeta-Regression\n\nmodel results\n\nresults of the model, estimated coefficients, etc (table)\nwhat features to add in the model\ninterpretation of results in the table\ntesting of assumptions (link to appendix?)\n\n\n\nmodel fit\n\npredictions vs observations\nmake table of the features and the predicted outcome (Klingwort&Toepoel)\n\n\n\n\n\nReporting biases\n\nfunnel plots? TO DO: look at what is appropriate\n\n\n\nCertainty of evidence",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "writing/discussion.html",
    "href": "writing/discussion.html",
    "title": "4  Discussion",
    "section": "",
    "text": "Conclusion",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Discussion</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Borenstein, Michael, ed. 2013. Introduction to Meta-Analysis.\nNachdr. Chichester: Wiley.\n\n\nBurke, Marshall, Anne Driscoll, David B. Lobell, and Stefano Ermon.\n2021. “Using Satellite Imagery to Understand and Promote\nSustainable Development.” Science 371 (6535): eabe8628.\nhttps://doi.org/10.1126/science.abe8628.\n\n\nChen, Guolong, Ruixia Yang, Xiangli Zhao, Lanyi Li, Lei Luo, and Honghao\nLiu. 2023. “Bibliometric Analysis of\nSpatial Technology for World\nHeritage: Application, Trend and\nPotential Paths.” Remote\nSensing 15 (19): 4695. https://doi.org/10.3390/rs15194695.\n\n\nHall, Ola, Francis Dompae, Ibrahim Wahab, and Fred Mawunyo Dzanku. 2023.\n“A Review of Machine Learning and Satellite Imagery for Poverty\nPrediction: Implications for Development Research and\nApplications.” Journal of International Development 35\n(7): 1753–68. https://doi.org/10.1002/jid.3751.\n\n\nHolloway, Jacinta, and Kerrie Mengersen. 2018. “Statistical\nMachine Learning Methods and\nRemote Sensing for Sustainable\nDevelopment Goals: A\nReview.” Remote Sensing 10 (9): 1365. https://doi.org/10.3390/rs10091365.\n\n\nKhatami, Reza, Giorgos Mountrakis, and Stephen V. Stehman. 2016a.\n“A Meta-Analysis of Remote Sensing Research on Supervised\nPixel-Based Land-Cover Image Classification Processes:\nGeneral Guidelines for Practitioners and Future\nResearch.” Remote Sensing of Environment 177 (May):\n89–100. https://doi.org/10.1016/j.rse.2016.02.028.\n\n\n———. 2016b. “A Meta-Analysis of Remote Sensing Research on\nSupervised Pixel-Based Land-Cover Image Classification Processes:\nGeneral Guidelines for Practitioners and Future Research.”\nRemote Sensing of Environment 177 (May): 89–100. https://doi.org/10.1016/j.rse.2016.02.028.\n\n\nLajeunesse, Marc J. 2016. “Facilitating Systematic Reviews, Data\nExtraction, and Meta-Analysis with the Metagear Package for r” 7:\n323–30.\n\n\nMachicao, J., A. Ben Abbes, L. Meneguzzi, P. L. P. Corrêa, A. Specht, R.\nDavid, G. Subsol, et al. 2022. “Mitigation Strategies\nto Improve Reproducibility of\nPoverty Estimations From\nRemote Sensing Images\nUsing Deep Learning.”\nEarth and Space Science 9 (8): e2022EA002379. https://doi.org/10.1029/2022EA002379.\n\n\nMagnussen, Steen. 2021. “Calibration of a Confidence Interval for\na Classification Accuracy.” Open Journal of Forestry 11\n(1): 14–36. https://doi.org/10.4236/ojf.2021.111002.\n\n\nPage, Matthew J, Joanne E McKenzie, Patrick M Bossuyt, Isabelle Boutron,\nTammy C Hoffmann, Cynthia D Mulrow, Larissa Shamseer, et al. 2021.\n“The PRISMA 2020 Statement: An Updated Guideline for Reporting\nSystematic Reviews.” BMJ, March, n71. https://doi.org/10.1136/bmj.n71.\n\n\nShi, Haiyang, Geping Luo, Olaf Hellwich, Mingjuan Xie, Chen Zhang, Yu\nZhang, Yuangang Wang, et al. 2022. “Variability and Uncertainty in\nFlux-Site-Scale Net Ecosystem Exchange Simulations Based on Machine\nLearning and Remote Sensing: A Systematic Evaluation.”\nBiogeosciences 19 (16): 3739–56. https://doi.org/10.5194/bg-19-3739-2022.\n\n\nThapa, Aakash, Teerayut Horanont, Bipul Neupane, and Jagannath Aryal.\n2023. “Deep Learning for Remote\nSensing Image Scene\nClassification: A Review and\nMeta-Analysis.” Remote Sensing\n15 (19): 4804. https://doi.org/10.3390/rs15194804.\n\n\nUN DESA. 2023. The Sustainable Development\nGoals Report 2023: Special\nEdition. The Sustainable\nDevelopment Goals Report. United\nNations. https://doi.org/10.18356/9789210024914.\n\n\nYin, Chun, Ningyezi Peng, Yuchen Li, Yuanyuan Shi, Shujuan Yang, and\nPeng Jia. 2023. “A Review on Street View Observations in Support\nof the Sustainable Development Goals.” International Journal\nof Applied Earth Observation and Geoinformation 117 (March):\n103205. https://doi.org/10.1016/j.jag.2023.103205.\n\n\nZhang, Yuzhen, Jingjing Liu, and Wenjuan Shen. 2022. “A\nReview of Ensemble Learning\nAlgorithms Used in Remote\nSensing Applications.” Applied\nSciences 12 (17): 8654. https://doi.org/10.3390/app12178654.",
    "crumbs": [
      "References"
    ]
  }
]